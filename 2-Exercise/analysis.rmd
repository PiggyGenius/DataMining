
# Data mining: $2^{nd}$ practical

### 1 : Data 

Number of unnamed columns : 
```{r, echo=FALSE}
NAm2 = read.table("NAm2.txt", header=TRUE)
print(sum(is.na(colnames(NAm2, do.NULL=TRUE))))
```

```{r}
# get the list of the tribe names
names = unique(NAm2$Pop)
# get the differents couples (latitude, longitude) for each tribe
npop = length(names)
# unique() keeps only one examplar for each
coord = unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
# colors for the plot
colPalette = rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
# set the icons for the plot
pch = rep(c(16,15,25),each = 9)
plot(coord[,c("long","lat")],pch = pch,col = colPalette,asp = 1)
# asp allows to have the correct ratio between axis longitude and latitude
# Then the map is not deformed
legend("bottomleft",legend = names,col = colPalette,lty = -1,pch = pch,cex = .75,ncol = 2,lwd = 2)
library(maps)
# add the map of the world
map("world",add = T)
```

### 2 : Regression
```{r, echo=FALSE}
NAaux = NAm2[,-c(1:7)]
NAauxLM = lm(formula = long ~ ., data = NAaux)
```
**Using all genetic markers as predictors, predict the longitude using a linear regression model. What happens ?**

There is more variables than data : we solve a linear system in which there is more unknown variables than equations. So there is an infinite number of solutions, that is why we have NA values.


### 3 : PCA

**A) Explain quickly the principle of PCA.**

PCA is an algorithm that projects a space into another space, that can be of a lesser dimension, while maximizing the explained variance, the algorithm considers that the data is linearly seperable and if it isn't it is possible to use kernel PCA.
The algorithm can be used on the feature space and the data space and we can resume it algorithm as:

* Standardize the D-dimensional dataset (PCA is highly sensible to scale differences).

* Construct the covariance matrix.

* Find the eigenpairs of the covariance matrix (eigenvectors and eigenvalues).

* Select the K first eigenvectors that correspond to the K largest eigenvalues where K is the dimensionality of the new space (K <= D).

* Construct a projection matrix from the top K eigenvectors.

* Transform the D-Dimensional dataset using the projection matrix to obtain the new K-dimensional space.

**B) Perform PCA on genetic data with all samples, do we need to use the argument scale of prcomp ?**

We don't need to use the scale argument since our data is already on the same scale (binary values). Scaling the data would even deterior the prediction since we would loose information.
TODO: Say more about why scaling woud be worst.

**C) Describe and interpret the obtained graphs, what populations are easily identified using the first two axes. Answer to the same question using the 5th and 6th axes.**

BON

```{r, echo=FALSE}
pcaNAm2 = prcomp(NAm2[,-c(1:8)], center = TRUE, scale = FALSE)
pca_plot <- function(caxes){
	plot(pcaNAm2$x[,caxes],col="white")
	for (i in 1:npop) {
		# print(names[i])
		lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",col=colPalette[i],pch=pch[i])
	}
	legend("top",legend = names,col = colPalette,lty = -1,pch = pch,cex = .75,ncol = 3,lwd = 2)
}
pca_plot(c(1,2))
pca_plot(c(5,6))
```

### 4 : PCR
#### a) Predict the latitude and longitude using the scores of the 250 PCA axes. Let denote the results of these regressions by *lmlat* and *lmlong*.

```{r, echo=FALSE}
pca250 = pcaNAm2$x[,1:250]

lmlong = lm(formula = long ~ ., data = cbind(pca250, NAm2["long"]))
lmlat = lm(formula = lat ~ ., data = cbind(pca250, NAm2["lat"]))

plot(lmlong$fitted.values,lmlat$fitted.values, col="white", asp=1)
for (i in 1:npop) {
	# print(names[i])
	lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],
		  lmlat$fitted.values[which(NAm2[,3]==names[i])],
		  type="p",col=colPalette[i],pch=pch[i])
}
legend("topright",legend=names,col=colPalette,
	   lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
map("world",add=T)
```

#### b) Calculate the mean error of the previous model built using 250 axes.
```{r, echo=FALSE, message=FALSE}
library(fields)
```

Mean error on the previous model using the orthodromic distance :
```{r, echo=FALSE}
longlat_found = data.frame(long=lmlong$fitted.values, lat=lmlat$fitted.values)
longlat_exp = NAm2[, c("long", "lat")]
print(mean(rdist.earth.vec(longlat_found, longlat_exp, miles=F)))
```
This error is expressed in kilometers (to compare it, America is about 13 000 km long from north to south).

### 5 : PCR and Cross-Validation

#### b)
##### i. Create an empty matrix predictedCoord with 2 columns ("longitude", "latitude") and as many rows as there are individuals.
```{r}
predictedCoord = matrix(nrow=nrow(NAm2), ncol=2, dimnames=list(c(), c("long", "lat")))
```

##### ii. Using as predictors the scores of the first 4 PCA axes, explain latitude and longitude using the individuals who do not belong to validation set n. 1.
```{r}
naxes = 4

pcalong = data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x))
pcalat = data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x))

# lmlong2 = lm(formula = long ~ ., data = pcalong, subset = )
# lmlat2 = lm(formula = lat ~ ., data = pcalat, subset = )
```

##### iii. Using the built model, predict latitude and longitude for individuals belonging to the validation set n. 1. Store the predicted coordinates into predictCoord

##### iiii. Repeat for all other validation sets. Calculate the prediction error.

### 6 : Conclusion
