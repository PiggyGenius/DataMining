
# Data mining: $2^{nd}$ practical

### 1 : Data 

Number of unnamed columns : 
```{r, echo=FALSE}
NAm2 = read.table("NAm2.txt", header=TRUE)
print(sum(is.na(colnames(NAm2, do.NULL=TRUE))))
```

```{r}
# get the list of the tribe names
names = unique(NAm2$Pop)
# get the differents couples (latitude, longitude) for each tribe
npop = length(names)
# unique() keeps only one examplar for each
coord = unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
# colors for the plot
colPalette = rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
# set the icons for the plot
pch = rep(c(16,15,25),each = 9)
plot(coord[,c("long","lat")],pch = pch,col = colPalette,asp = 1)
# asp allows to have the correct ratio between axis longitude and latitude
# Then the map is not deformed
legend("bottomleft",legend = names,col = colPalette,lty = -1,pch = pch,cex = .75,ncol = 2,lwd = 2)
library(maps)
# add the map of the world
map("world",add = T)
```

### 2 : Regression
```{r, echo=FALSE}
NAaux = NAm2[,-c(1:7)]
NAauxLM = lm(formula = long ~ ., data = NAaux)
```
**Using all genetic markers as predictors, predict the longitude using a linear regression model. What happens ?**

There is more variables than data : we solve a linear system in which there is more unknown variables than equations. So there is an infinite number of solutions, that is why we have NA values.


### 3 : PCA

**A) Explain quickly the principle of PCA.**

PCA is an algorithm that projects a space into another space, that can be of a lesser dimension, while maximizing the explained variance, the algorithm considers that the data is linearly seperable and if it isn't it is possible to use kernel PCA.
The algorithm can be used on the feature space and the data space and we can resume it as:

* Standardize the D-dimensional dataset (PCA is highly sensible to scale differences).

* Construct the covariance matrix.

* Find the eigenpairs of the covariance matrix (eigenvectors and eigenvalues).

* Select the K first eigenvectors that correspond to the K largest eigenvalues where K is the dimensionality of the new space (K <= D).

* Construct a projection matrix from the top K eigenvectors.

* Transform the D-Dimensional dataset using the projection matrix to obtain the new K-dimensional space.

**B) Perform PCA on genetic data with all samples, do we need to use the argument scale of prcomp ?**

We don't need to use the scale argument since our data is already on the same scale (binary values). Scaling the data would even deterior the prediction since we would loose information.
We can confirm that by comparing our PCA with feature scaling and PCA without feature scaling, in the case of PCA without scaling our first two components capture 37.7% of the variance. For PCA with scaling, our first two components only capture around 0.3% of the variance.

```{r, echo=FALSE}
genetic_data = NAm2[,-c(1:8)]
pcaNAm2 = prcomp(genetic_data, center = FALSE, scale = FALSE)
```

**C) Describe and interpret the obtained graphs, what populations are easily identified using the first two axes. Answer to the same question using the 5th and 6th axes.**

TODO.
```{r, echo=FALSE}
pca_plot <- function(caxes){
	plot(pcaNAm2$x[,caxes],col="white", ylim=c(min(pcaNAm2$x[, caxes[2]]), max(pcaNAm2$x[, caxes[2]]) + 20))
	for (i in 1:npop) {
		# print(names[i])
		lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",col=colPalette[i],pch=pch[i])
	}
	legend("top", legend = names,col = colPalette, lty = -1,pch = pch,cex = .75,ncol = 3,lwd = 2)
}
pca_plot(c(1,2))
pca_plot(c(5,6))
```

**D) What percentage of variance is captured by the first two principle components ? How many principle components would you keep if you would like to represent genetic markers using a minimal number of components ?**

The first and second component capture around 37,7% of the variance. The number of components depends on how much of the variance we are willing to loose, if we consider that 80% percent of the variance is enough, we would keep the 210 first components.

```{r, echo = TRUE}
variance_proportion = pcaNAm2$sdev^2
variance_proportion = variance_proportion / sum(variance_proportion)
print(cumsum(variance_proportion[1:2])[2])
print(cumsum(variance_proportion[1:210])[210])
```


### 4 : PCR
#### a) Predict the latitude and longitude using the scores of the 250 PCA axes. Let denote the results of these regressions by *lmlat* and *lmlong*.

```{r, echo=FALSE}
pca250 = pcaNAm2$x[,1:250]

lmlong = lm(formula = long ~ ., data = cbind(pca250, NAm2["long"]))
lmlat = lm(formula = lat ~ ., data = cbind(pca250, NAm2["lat"]))

plot(lmlong$fitted.values,lmlat$fitted.values, col="white", asp=1, ylim=c(min(lmlat$fitted.values), max(lmlat$fitted.values) + 120))
for (i in 1:npop) {
	# print(names[i])
	lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],
		  lmlat$fitted.values[which(NAm2[,3]==names[i])],
		  type="p",col=colPalette[i],pch=pch[i])
}
legend("topright",legend=names,col=colPalette,
	   lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
map("world",add=T)
```

#### b) Calculate the mean error of the previous model built using 250 axes.
```{r, echo=FALSE, message=FALSE}
library(fields)
```

Mean error on the previous model using the orthodromic distance :
```{r, echo=FALSE}
longlat_found = data.frame(long=lmlong$fitted.values, lat=lmlat$fitted.values)
longlat_exp = NAm2[, c("long", "lat")]
print(mean(rdist.earth.vec(longlat_found, longlat_exp, miles=F)))
```
This error is expressed in kilometers (to compare it, America is about 13 000 km long from north to south).

### 5 : PCR and Cross-Validation

#### a) Recall the principle of cross-validation method. Explain why this method is interesting to build a predictive model. Create a vector set that contains for each individual the index of the subset he/she belongs to.

Cross-validation is a technique used to minimize the bias introduced by split-validation that only uses one subset of the data to test the model (depending on which subset we might have very different performance estimates). Cross-validation can be resumed as follows:

* Randomly split the data into K-folds without replacement.

* K-1 folds are used for model training and one fold is used for testing.

* Repeat the preceding operation K times with a different testing fold so that we obtain K models and performance estimates.

* Calculate the average performance of the models to obtain a performance estimate that is less sensitive to the partitioning of the data than split-validation.

```{r, echo = TRUE}
set_count = 10
set = sample(rep(1:set_count, each = nrow(genetic_data)/set_count))
```

#### b)

##### i. Create an empty matrix predictedCoord with 2 columns ("longitude", "latitude") and as many rows as there are individuals.
```{r}
predictedCoord = matrix(nrow=nrow(NAm2), ncol=2, dimnames=list(c(), c("long", "lat")))
```

##### ii. Using as predictors the scores of the first 4 PCA axes, explain latitude and longitude using the individuals who do not belong to validation set n. 1.
##### iii. Using the built model, predict latitude and longitude for individuals belonging to the validation set n. 1. Store the predicted coordinates into predictCoord
##### iiii. Repeat for all other validation sets. Calculate the prediction error.

```{r}
pcalong = data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x))
pcalat = data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x))

predict_longlat = function(naxes) {
	# naxes : number of PCA axes to use
	# returns the mean of the predicted error

	predictedCoord = matrix(nrow=nrow(NAm2), ncol=2, dimnames=list(c(), c("long", "lat")))
	set = sample(rep(1:set_count, each = nrow(genetic_data)/set_count))

	pcalong_naxes = pcalong[,1:(naxes+1)]
	pcalat_naxes = pcalat[,1:(naxes+1)]

	for (i in 1:set_count) {

		train = which(set != i)
		lmlong2 = lm(formula = long ~ ., data = pcalong_naxes, subset = train)
		lmlat2 = lm(formula = lat ~ ., data = pcalat_naxes, subset = train)

		predictedCoord[-train,"long"] = predict(lmlong2, pcalong_naxes[-train,])
		predictedCoord[-train,"lat"] = predict(lmlat2, pcalat_naxes[-train,])
	}

	return (mean(rdist.earth.vec(predictedCoord, longlat_exp, miles=F)))
}
```

#### c) Repeat all steps of b), changing naxes from 2 to 440. Plot the prediction errors and the error obtained on the training set versus the number of components.


```{r}
# pcamin = 2
# pcamax = 440
# plot(pcamin:pcamax, lapply(pcamin:pcamax, predict_longlat))
```





### 6 : Conclusion
